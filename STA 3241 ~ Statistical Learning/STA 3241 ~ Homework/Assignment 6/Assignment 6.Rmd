---
title: "R Notebook"
author: "Gus Lipkin ~ glipkin6737@floridapoly.edu"
output: html_notebook
---

You are provided the data on home sales in a mid-western city that includes 522 observations. The variables in the dataset in order are sales price ($), finished area of the residence (square feet), number of bedrooms and bathrooms, presence of air conditioning and pool, number of cars the garage will hold, quality of construction (low, medium or high), architectural style, lot size (square feet) and presence or absence of adjacency to a highway. 

Please use the following methods to determine the predictors that drive the sale price of a home

```{r}
library(data.table)
library(caret)
library(rpart.plot)
library(tree)
library(randomForest)
library(gbm)
```

```{r}
dt <- fread("APPENC07_Age.txt", col.names = c("salePrice", "sqFt", "bedrooms", "bathrooms", "ac", "cars", "pool", "quality", "style", "lotSize", "highway", "houseAge"))

intToBool <- function(x) {
  ifelse(x == 1, TRUE, FALSE)
}

dt$ac <- sapply(dt$ac, intToBool)
dt$pool <- sapply(dt$pool, intToBool)

dt$bedrooms <- as.factor(dt$bedrooms)
dt$bathrooms <- as.factor(dt$bathrooms)
dt$cars <- as.factor(dt$cars)
dt$quality <- as.factor(dt$quality)
dt$style <- as.factor(dt$style)
dt$highway <- as.factor(dt$highway)

head(dt)
```

```{r}
set.seed(2022)
rowPicker <- createDataPartition(y=dt$salePrice, p=0.8, list=FALSE)
train <- dt[rowPicker]
test <- dt[-rowPicker]

control <- trainControl(method = "cv", number = 10)
```

# Decision Trees
```{r}
rpartTree <- train(salePrice ~ ., train, method = "rpart")
rpart.plot(rpartTree$finalModel)
yHatRpart <- predict(rpartTree, test)
summary(yHatRpart)
plot(yHatRpart, test$salePrice)
abline(0, 1)
mseRpart <- mean(yHatRpart - test$salePrice)^2
```

```{r}
regularTree <- tree(salePrice ~ ., train)
yHatRegular <- predict(regularTree, test)
summary(yHatRegular)
plot(yHatRegular, test$salePrice)
abline(0, 1)
text(regularTree, pretty = 0)
mseTree <- mean(yHatRegular - test$salePrice)^2
```

# Bagging

```{r}
bag <- randomForest(salePrice ~ ., data = train, mtry = 8)
bag
yHatBag <- predict(bag, test)
plot(yHatBag, test$salePrice)
abline(0, 1)
mseBagging <- mean(yHatBag - test$salePrice)^2
```

# Random Forests

```{r}
rf <- randomForest(salePrice ~ ., train, mtry = 8, importance = TRUE)
rf
yHatRf <- predict(bag, test)
plot(yHatRf, test$salePrice)
abline(0, 1)
mseRf <- mean(yHatRf - test$salePrice)^2

importance(rf)
varImpPlot(rf)
```

```{r}
tunegrid <- expand.grid(mtry = c(8))
bag2 <- train(salePrice ~ ., train, tuneGrid = tunegrid, method = "rf", importance = TRUE)
yHatBag2 <- predict(bag2$finalModel, newdata = test)
plot(yHatBag2, test$salePrice)
abline(0, 1)
mseBag2 <- mean(yHatBag2 - test$salePrice)^2
varImp(bag2$finalModel)
```

# Boosting

```{r}
dt$ac <- as.factor(dt$ac)
dt$pool <- as.factor(dt$pool)
set.seed(2022)
rowPicker <- createDataPartition(y=dt$salePrice, p=0.8, list=FALSE)
train <- dt[rowPicker]
test <- dt[-rowPicker]

boost <- gbm(salePrice ~ ., train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost)
plot(boost, i = "quality")
plot(boost, i = "sqFt")
yHatBoost <- predict(boost, test, n.trees = 5000)
plot(yHatBoost, test$salePrice)
abline(0, 1)
mseBoost <- mean(yHatBoost - test$salePrice)^2
```

# Provide a comparison of the test MSE for the above methods. 
```{r}
mse <- c("Bagging" = mseBagging, 
  "Boosting" = mseBoost, 
  "Random Forest" = mseRf, 
  "RPart Tree" = mseRpart,
  "Tree" = mseTree)
mse[order(mse)]
```

