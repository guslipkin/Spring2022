---
title: "Linear Regression"
output: html_notebook
---

If you have not used any of the below packages before please install the package through the console.We load the MASS (a large collection of data sets and functions) and ISLR2 package (data sets from the textbook).  

```{r}
library(MASS)
library(ISLR2)
library(tidyverse)
library(data.table)
```

Download Advertising.csv from canvas

```{r}
ad <- fread("Data/Advertising.csv")
head(ad)
```
Summary statistics

```{r}
summary(ad)
```
Attach the data set to easily reference the variable names
```{r}
attach(ad)
```

## Simple Linear Regression Model

Linear regression models are a useful tool for predicting a quantitative response.
Goal is to establish a relation between Sales and money spent on advertising for either TV, Radio or Newspaper. 
We can create a simple linear model of the form y=β0+β1x+ϵ, where the response/dependent variable y is sales, and the independent/explanatory variable x represents money spent in advertising in TV, Radio or newspaper.

```{r}
TV_lm <- lm(sales ~ TV, ad)
Radio_lm <- lm(sales ~ radio, ad)
Newspaper_lm <- lm(sales ~ newspaper, ad)
```

Plot the regression lines 

```{r}
par(mfrow = c(1, 3))
plot(TV, sales)
abline(TV_lm, col = "red", lwd = 4)
plot(radio, sales)
abline(Radio_lm, col = "red", lwd = 4)
plot(newspaper, sales)
abline(Newspaper_lm, col = "red", lwd = 4)
```
Let us focus on one model Sales vs TV

```{r}
summary(TV_lm)
```

```{r}

```
Coefficients:

Theoretically, in simple linear regression, the coefficients are two unknown constants that represent the intercept and slope terms in the linear model. If we wanted to predict the sales for an advertising agency given the money it spent on advertising in TV, we would get a training set and produce estimates of the coefficients to then use it in the model formula.Ultimately, the analyst wants to find an intercept and a slope such that the resulting fitted line is as close as possible to the data points in our data set.

Significance test

In the linear regression model y=β0+β1x+ϵ, we can decide whether there is any significant relationship between x and y by testing the null hypothesis that β1=0.

The significance test studies the null hypothesis H0 that claims that there is no relationship between x and y, that is

H0:β1=0

H1:β1≠0

Notice that if β1=0 then y=β0+ϵ.

Standard  Error
The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable.  We would ideally want a lower number relative to its coefficients.
In  our  example,  we  have  previously  determined  that  for  every  one dollar 1  increase  in  the  amount spent on advertising in TV,  the sales go up by 0.04 dollars.  The Standard Error can be used to compute an estimate of the expected difference in case we ran the model again and again.The Standard Errors can also be used to compute confidence intervals and to statistically test the hypothesis of the existence of a relationship between the dependent and independent variables."

t value
The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0.  We want it to be far away from zero as this would indicate we could reject the null hypothesis - that is, we could declare a relationship between depndent and independent variable exists.
In our example, the t-statistic values are relatively far away from zero and are large relative to the standard error, which could indicate a relationship exists.  In general, t-values are also used to compute p-values.


Pr(>t)
The Pr(>t) acronym found in the model output relates to the probability of observing any value equal or larger than t.  A small p-value indicates that it is unlikely we will observe a relationship between the predictor (speed) and response (dist) variables due  to  chance.
Typically, a p-value of 5% or less is a good cut-off point.  In our model example, the p-values are very close to zero.  Note the Signif. codes associated to each estimate.  Three stars (or asterisks) represent a highly significant p-value.  Consequently, a small p-value for the intercept and the slope indicates that we can reject the null hypothesis which allows us to conclude that there is a relationship between speed and distance.

"Residual  Standard  Error
Residual Standard Error is a measure of the quality of a linear regression fit.  Theoretically,  every linear model is assumed to contain an error term ε.  Due to the presence of this error term, we are not capable of
perfectly predicting  our response variable (dist) from the predictor (speed) one.
The  Residual  Standard  Error  is  the  average  amount  that  the  response  (dist)  will  deviate  from  the  true regression line.  In our example, the sales can deviate from the true regression line by approximately 17.67 feet, on average.


"Multiple  R-squared,  Adjusted  R-squared
The R-squared (R2) statistic provides a measure of how well the model is fitting the actual data.  It takes the form of a proportion  of  variance.  R2 is a measure of the linear relationship between our predictor variable (speed) and our response / target variable (dist).  It always lies between 0 and 1 (i.e.:  a number
near 0 represents a regression that does not explain the variance in the response variable well and a numberclose to 1 does explain the observed variance in the response variable).
In our example,  the R2  we get is 0.6119.  Or roughly 61% of the variance found in the response variable (dist) can be explained by the predictor variable (speed).
What R-Squared tells us is the proportion of variation in the dependent (response) variable that has been explained by this model."

A side note:  In multiple regression settings, the R2 will always increase as more variables are included in the model.  That is why the adjusted R2 is the preferred measure as it adjusts for the number of variables considered."
	

"F-Statistic
F-statistic  is  a  good  indicator  of  whether  there  is  a  relationship  between  our  predictor  and  the  response variables.  The further the F-statistic is from 1 the better it is.  However, how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors.
Generally, when the number of data points is large, an F-statistic that is only a little bit larger than 1 is already sufficient to reject the null hypothesis (H0 :  There is no relationship between sales and TV).
The  reverse  is  true  as  if  the  number  of  data  points  is  small,  a  large  F-statistic  is  required  to  be  able  to ascertain that there may be a relationship between predictor and response variables.  In our example the F-statistic  is relatively larger than 1 given the size of our data.

## Multiple Linear Regression Example 1

Here our model is given by

y=β0+β1x1+β2x2+β3x3+ϵ

We can interpret βj as the average effect on y of a one unit increase in xj, holding all other predictors fixed.

correlations between the variables

```{r}
cor(ad)
```

create a pairs plot

```{r}
pairs(ad, col = "blue", pch=20)
```

```{r}
lm <- lm(sales ~ TV + radio + newspaper, ad)
lm
summary(lm)
```
regression model with all the variables

```{r}

```
## Multiple Linear Regression Example 2 

The `ISLR2` library contains the `Boston`  data set, which
records `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).

```{r}
head(Boston)
```
Estimate a regression model using all the predictors

```{r}
attach(Boston)
lm_all <- lm(medv ~ ., data = Boston)
lm_all
summary(lm_all)
```

Perform regression using all variables but one. In the above, we see that `indus` and age have a high $p$-value and wish to run a model without one of these variables. 

```{r}
lm_all_1 <- lm(medv ~ . -indus, data = Boston)
lm_all_1
summary(lm_all_1)
```
Alternatively, the `update()` function can be used.

```{r}
lm_all_2 <- update(lm_all_1, ~. -age)
lm_all_2
summary(lm_all_2)
```

## VIF

We can access the individual components of a summary object by name
(type `?summary.lm` to see what is available). Hence
`summary(lm.fit)$r.sq` gives us the $R^2$, and
`summary(lm.fit)$sigma` gives us the RSE. The `vif()`
function, part of the `car` package, can be used to compute variance
inflation factors.   Most VIF's are
low to moderate for this data. The `car` package is not part of the base `R` installation so it must be downloaded the first time you use it via the `install.packages()` function in `R`.

```{r chunk16}
library(car)
vif(lm_all_2)
```
## Interaction Terms

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells `R` to include an interaction term between `lstat` and `black`.
The syntax `lstat * age` simultaneously includes `lstat`, `age`, and the interaction term `lstat`$\times$`age` as predictors; it is a shorthand for `lstat + age + lstat:age`.
  %We can also pass in transformed versions of the predictors.

```{r chunk19}
summary(lm(medv ~ crim + lstat:age, data = Boston))
summary(lm(medv ~ crim + lstat*age, data = Boston))
```

## Interaction on Sales Data

```{r}
sales_multlm_2 <- lm(sales ~ TV*radio + newspaper, data = ad)
```

```{r}
sales_multlm_2 <- lm(sales ~ TV *radio)
summary(sales_multlm_2)
```


## Non-linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor $X$, we can create a predictor $X^2$ using
 `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula object; wrapping as we do allows the standard usage in `R`, which is to raise `X` to the power `2`. We now
perform a regression of `medv` onto `lstat` and `lstat^2`.

```{r chunk20}
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
```

The near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model.
We use the `anova()` function  to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{r chunk21}
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat^2`.
The `anova()` function performs a hypothesis test
comparing the two models. The   null hypothesis is that the two models fit the data equally well,  and the alternative hypothesis is that the full model is superior. Here the $F$-statistic is $135$
 and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`.
 This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type

```{r chunk22}
par(mfrow = c(2, 2))
plot(lm.fit2)
```

 then we see that when the `lstat^2` term is included in the model, there is little discernible pattern in the residuals.

In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a
fifth-order polynomial fit:

```{r chunk23}
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
```

This suggests that including additional  polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant $p$-values
in a regression fit.

 By default, the `poly()` function orthogonalizes the predictors:
 this means that the features output by this function are not simply a
 sequence of powers of the argument. However, a linear model applied to the output of the `poly()` function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the `poly()` function,  the argument `raw = TRUE` must be used.

Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.

```{r chunk24}
summary(lm(medv ~ log(rm), data = Boston))
```


## Qualitative Predictors

We will now examine the `Carseats` data, which is part of the
`ISLR2` library. We will  attempt to predict `Sales`
(child car seat sales) in $400$ locations based on a number of
predictors.

```{r chunk25}
head(Carseats)
```

The `Carseats` data includes qualitative predictors such as `shelveloc`, an indicator of the quality of the shelving location---that is, the  space within a store in which the car seat is displayed---at each location. The predictor `shelveloc` takes on three possible values:  *Bad*, *Medium*, and *Good*. Given a qualitative variable such as `shelveloc`, `R` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.

```{r chunk26}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, 
    data = Carseats)
summary(lm.fit)
```

The `contrasts()` function returns the coding that `R` uses for the dummy variables.

```{r chunk27}
attach(Carseats)
contrasts(ShelveLoc)
```

Use `?contrasts` to learn about other contrasts, and how to set them.

`R` has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise.
A bad shelving location corresponds to a zero for each of the two dummy variables.
The fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.






